{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/bert_nlp/blob/main/section_5/01_news_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrgegdDZjf8E"
      },
      "source": [
        "# 学術分野の分類（複数）\n",
        "## jst_categorize_multi.ipynb\n",
        "kakenhi_fine_tuning.ipynbでファインチューニングにより作成したモデルを用いて、新たな複数の学術文書（JSTやAMEDを含めた科研費に限らない）を11の大区分に分類するプログラム。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6moZnLFkFwr"
      },
      "source": [
        "### ライブラリのインストール\n",
        "SageMakerでは初回だけで良い（Google colaboratoryでは毎回）  \n",
        "ライブラリTransformers、およびnlpをインストールします。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipのアップデート（たまに走らせても良いかも）\n",
        "!pip list\n",
        "!python -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qg6t5nnBjqs"
      },
      "outputs": [],
      "source": [
        "# ライブラリのインストール\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install nlp\n",
        "!pip install datasets\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcHOX9LyZc2g"
      },
      "source": [
        "### Google ドライブとの連携  \n",
        "以下のコードを実行し、認証コードを使用してGoogle ドライブをマウントします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h7BA67Ed5wT"
      },
      "outputs": [],
      "source": [
        "# SageMakerでは不要\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 複数の学術文書を分類\n",
        "学習済み（ファインチューニング済み）のモデルを読み込み、複数の学術文書（実際には科研費概要テキスト）を分類する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZuJCZBK0RJx"
      },
      "source": [
        "### モデルの読み込み\n",
        "保存済みのモデルを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWtcQRuP0X45"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
        "\n",
        "#data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "data_path = \"./\" # SageMaker\n",
        "\n",
        "loaded_model = BertForSequenceClassification.from_pretrained(data_path) \n",
        "# loaded_model.cuda() # GPU対応\n",
        "loaded_tokenizer = BertJapaneseTokenizer.from_pretrained(data_path)\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 分類精度検証用の科研費データ読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 科研費データベースからダウンロードした未加工のcsvファイルを指定\n",
        "# open_original_csv = \"KibanC_2021_Original.csv\"\n",
        "open_original_csv = \"KibanC_2022-2022.csv\" # 直近の１年（2022年）\n",
        "# open_original_csv = \"KibanC_2019-2020.csv\" # 2018は「研究開始時の研究の概要」が無い\n",
        "#data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "data_path = \"../data/\"\n",
        "\n",
        "# csvファイルを開く\n",
        "raw_data2 = pd.read_csv(data_path + open_original_csv) # dtype=\"object\"必要？\n",
        "\n",
        "# 読み込んだデータをチェック\n",
        "# raw_data.info()\n",
        "\n",
        "\n",
        "raw_data2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 科研費データベースからダウンロードした未加工のcsvファイルを指定\n",
        "# open_original_csv = \"KibanC_2021_Original.csv\"\n",
        "open_original_csv = \"KibanC_2022-2022.csv\" # 直近の１年（2022年）\n",
        "# open_original_csv = \"KibanC_2019-2020.csv\" # 2018は「研究開始時の研究の概要」が無い\n",
        "#data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "data_path = \"../data/\"\n",
        "\n",
        "# csvファイルを開く\n",
        "raw_data2 = pd.read_csv(data_path + open_original_csv) # dtype=\"object\"必要？\n",
        "\n",
        "# 読み込んだデータをチェック\n",
        "# raw_data.info()\n",
        "\n",
        "# 今後必要な行だけを取り出し、リネーム\n",
        "kadai2 = raw_data2[[\"研究課題/領域番号\", \"審査区分\", \"研究開始時の研究の概要\"]]\n",
        "kadai2.columns = [\"ID\", \"ShoKubun\", \"Abst\"]\n",
        "\n",
        "# 課題番号の重複を確認。課題番号でソートする。\n",
        "kadai2[\"ID\"].duplicated().any()\n",
        "# kadai2 = kadai2.set_index(\"ID\") # IDをインデックスに設定するコード\n",
        "kadai2 = kadai2.sort_values(\"ID\")\n",
        "\n",
        "# Abstが空欄の課題を削除\n",
        "print(\"オリジナルの課題数： %5d\" % len(kadai2))\n",
        "print(\"概要が空白の課題数： %5d\" % len(kadai2[kadai2[\"Abst\"].isna()]))\n",
        "kadai2 = kadai2.dropna(subset=[\"Abst\"])\n",
        "print(\"空白を除いた課題数： %5d\" % len(kadai2))\n",
        "\n",
        "# Abst中の改行コードを削除\n",
        "kadai2 = kadai2.replace('\\r', '', regex=True)\n",
        "kadai2 = kadai2.replace('\\n', '', regex=True)\n",
        "\n",
        "# Abstが英語のみの課題を削除\n",
        "num_jpen = len(kadai2)\n",
        "kadai2 = kadai2[kadai2[\"Abst\"].str.contains(r'[ぁ-んァ-ン]')]\n",
        "num_jp   = len(kadai2)\n",
        "print(\"日本語＋英語： %5d\" % num_jpen)\n",
        "print(\"英語　　　　： %5d\" % (num_jpen - num_jp))\n",
        "print(\"日本語　　　： %5d\" % num_jp)\n",
        "\n",
        "# kadai.to_csv(data_path + \"test1.csv\", encoding = \"cp932\")\n",
        "\n",
        "# 小区分が設定されていない課題を削除（旧分類、特設分野）\n",
        "aaa = len(kadai2)\n",
        "kadai2 = kadai2.dropna(subset=[\"ShoKubun\"])\n",
        "print(\"小区分がブランク： %5d\" % (aaa - len(kadai2)))\n",
        "print(\"小区分の設定あり： %5d\" % len(kadai2))\n",
        "\n",
        "# 小区分の文字列の数字部分だけを取り出す\n",
        "kadai2[\"ShoKubun\"] = kadai2[\"ShoKubun\"].str[3:8]\n",
        "kadai2 = kadai2.astype({\"ShoKubun\": int})\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 大区分に変換"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 科研費の審査区分表データのcsvファイル\n",
        "open_kubun_csv = \"KubunTable.csv\"\n",
        "# data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "data_path = \"./\" # sagamaker\n",
        "\n",
        "# 審査区分テーブルのロード\n",
        "kubun_table = pd.read_csv(data_path + open_kubun_csv, encoding=\"cp932\")\n",
        "kubun_table = kubun_table[[\"tabDai\", \"tabSho\"]]\n",
        "\n",
        "# 審査区分表の重複を削除（一つの小区分が２つまたは３つの『中区分』に所属することに由来する）\n",
        "print(\"審査区分表の整理\")\n",
        "print(\"重複削除前の項目数： %3d\" % len(kubun_table))\n",
        "kubun_table = kubun_table.drop_duplicates()\n",
        "print(\"重複削除後の項目数： %3d\" % len(kubun_table))\n",
        "\n",
        "# 大区分への変換\n",
        "# mergeを用いて、審査区分表のデータと突合\n",
        "print(\"統合前のデータ数： %5d\" % len(kadai2))\n",
        "kadaiDai2 = pd.merge(kadai2, kubun_table, left_on='ShoKubun', right_on='tabSho')\n",
        "kadaiDai2 = kadaiDai2[[\"Abst\", \"tabDai\", \"ID\", \"ShoKubun\"]]\n",
        "print(\"統合したデータ数： %5d\" % len(kadaiDai2))\n",
        "\n",
        "kadaiDai2.info()\n",
        "\n",
        "show_num = 0\n",
        "kadaiDai2[\"Abst\"][show_num]\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 分類精度を複数ファイルで確認\n",
        "３時間程度かかるかも  \n",
        "sagemakerでは40分程度"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob  # ファイルの取得に使用\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "results_binary = 'results'\n",
        "# data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "data_path = \"./\" # sagemaker\n",
        "Daikubun = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"]\n",
        "\n",
        "\n",
        "max_length = 512\n",
        "\n",
        "num_data = len(kadaiDai2)\n",
        "print(\"分類する課題数： %d\" % num_data)\n",
        "# num_data = 100\n",
        "num_category = len(Daikubun)\n",
        "# results = torch.zeros(num_data, num_category) # テンソルで変数を用意\n",
        "results = np.zeros((num_data, num_category)) # メモリーが足りないと言われるのでテンソルではなくnumpy arrayにしてみた\n",
        "\n",
        "for m in tqdm(range(num_data)):\n",
        "  words = loaded_tokenizer.tokenize(kadaiDai2[\"Abst\"][m])\n",
        "  word_ids = loaded_tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "  word_tensor = torch.tensor([word_ids[:max_length]])  # テンソルに変換\n",
        "  \n",
        "  y = loaded_model(word_tensor) # GPU未対応時の予測\n",
        "  # y = loaded_model(word_tensor.cuda())  # GPU対応時の予測\n",
        "\n",
        "  # results[m,:] = y[0]\n",
        "  results[m,:] = y[0].detach().numpy() # テンソルをnumpy arrayに変換\n",
        "\n",
        "# 変数をとりあえずバイナリで保存\n",
        "np.save(data_path+results_binary, results) # 計算結果をとりあえずバイナリで保存\n",
        "\n",
        "print(results.shape)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ここで一度保存\n",
        "時間がかかる処理のあとなので、この段階で生成できたデータをファイルに保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# コードが不十分\n",
        "data_path = \"./\" # sagemaker\n",
        "Daikubun = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"]\n",
        "\n",
        "results = np.load(data_path+results_binary+\".npy\")\n",
        "\n",
        "# 変数をとりあえずバイナリで保存\n",
        "#np.save(data_path+results_binary, results) # 計算結果をとりあえずバイナリで保存\n",
        "np.savez(data_path+\"categolization_results\", results, kadaiDai2, Daikubun)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "分類精度を表示\n",
        "混同行列"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 分類結果を表示"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 分類精度を複数ファイルで確認\n",
        "confusion matrix （混同行列）を作成  \n",
        "「マルチラベリング」で対応する方法もありそう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "show_num = 0\n",
        "\n",
        "results_binary = \"results\"\n",
        "# data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "data_path = \"./\" # sagemaker\n",
        "Daikubun = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"]\n",
        "\n",
        "# num_category = len(Daikubun)\n",
        "\n",
        "results_2 = np.load(data_path+results_binary+\".npy\")\n",
        "\n",
        "#results_3 = np.load(data_path+\"categolization_results.npz\", allow_pickle=True)\n",
        "#results_2 = results_3['arr_0']\n",
        "#kadaiDai2 = results_3['arr_1']\n",
        "#Daikubun  = results_3['arr_2']\n",
        "\n",
        "print(\"ロードした推定確率データのサイズ %d %d\" % results_2.shape)\n",
        "\n",
        "num_category = len(Daikubun)\n",
        "\n",
        "results_2 = torch.tensor(results_2) # Softmax関数を使うためにテンソルに変換\n",
        "m = torch.nn.Softmax(dim=1) # Softmax関数で確率に変換\n",
        "results_2 = m(results_2)\n",
        "results_2 = results_2.numpy() # numpy arrayに戻す\n",
        "\n",
        "kadaiDai2['estimated'] = np.argmax(results_2, axis=1) # 最大要素のindexを返す（確率が最大の区分を返す）\n",
        "\n",
        "\n",
        "print(\"重複課題の削除前 %5d\" % len(kadaiDai2))\n",
        "kadaiDai3 = kadaiDai2.drop_duplicates(subset='ID', keep=False)\n",
        "print(\"重複課題の削除後 %5d\" % len(kadaiDai3))\n",
        "\n",
        "duplicated_data = kadaiDai3[kadaiDai3.duplicated(subset='ID', keep=False)]\n",
        "unique_id = duplicated_data['ID'].drop_duplicates()\n",
        "\n",
        "\n",
        "#\n",
        "#for kk in unique_id:\n",
        "#  dup_set = duplicated_data[duplicated_data['ID'] == kk]\n",
        "#  cat_est = dup_set['estimated'].tolist()[0]\n",
        "#  cat_real = dup_set['tabDai'].tolist()\n",
        "#\n",
        "#  if cat_est in cat_real:\n",
        "#    aaa = cat_real.index(cat_est)\n",
        "#    print(dup_set[:aaa])\n",
        "#  else:\n",
        "#    aaa = cat_est\n",
        "#\n",
        "#print(aaa)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://analysis-navi.com/?p=553\n",
        "confmat = confusion_matrix(kadaiDai3['tabDai'], kadaiDai3['estimated']) # 混同行列の取得。(true, predicted)の順番\n",
        "confmat = pd.DataFrame(confmat,columns=[\"pred_\" + str(l) for l in Daikubun], index=[\"act_\" + str(l) for l in Daikubun])\n",
        "# print(cm)\n",
        "\n",
        "\n",
        "#dup_idx_all = kadaiDai2.drop_duplicated(subset='ID',keep=False)\n",
        "\n",
        "#duplicated_data_all = kadaiDai2[dup_idx_all]\n",
        "#duplicated_data_all = duplicated_data_all[['ID', 'tabDai', 'estimated']]\n",
        "#print(duplicated_data_all.shape)\n",
        "#duplicated_data_all = duplicated_data_all.groupby('ID').count()\n",
        "\n",
        "#dup_idx_first = kadaiDai2[kadaiDai2.duplicated(subset='ID', keep='first')]\n",
        "#dup_idx_first = dup_idx_first.index\n",
        "\n",
        "#for kk in dup_idx_first:\n",
        "#  print(kk)\n",
        "\n",
        "\n",
        "#duplicated_data.to_csv(data_path+\"duplicated_mat.csv\")\n",
        "\n",
        "#kubun_estimated = np.argmax(results_2, axis=1)\n",
        "#kubun_real      = kadaiDai2[\"tabDai\"]\n",
        "#\n",
        "#kubun_results = pd.DataFrame(\n",
        "#    data={\n",
        "#        'estimated':kubun_estimated,\n",
        "#        'real':kubun_real\n",
        "#    }\n",
        "#)\n",
        "\n",
        "# 結果のグラフを表示\n",
        "plt.bar(Daikubun, results_2[show_num,:])\n",
        "print(kadaiDai3[\"Abst\"][show_num])\n",
        "print(kadaiDai3[\"ShoKubun\"][show_num])\n",
        "print(kadaiDai3['ID'][show_num])\n",
        "print(\"実際　大区分\" + Daikubun[kadaiDai3[\"tabDai\"][show_num]])\n",
        "print(\"推定　大区分\" + Daikubun[kadaiDai3[\"estimated\"][show_num]])\n",
        "print(int(results_2[show_num, kadaiDai3[\"estimated\"][show_num]]*100))\n",
        "#kadaiDai2 = kadaiDai2[[\"Abst\", \"tabDai\", \"ID\", \"ShoKubun\"]]\n",
        "\n",
        "#out_mat = kubun_results.value_counts(sort=False)\n",
        "#kubun_results.aggregate\n",
        "\n",
        "\n",
        "#out_mat.to_csv(data_path+\"result_mat.csv\")\n",
        "\n",
        "#dup_idx_first\n",
        "confmat"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM+jlwIO2oHuQ1CSlhpydtp",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "01_news_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
